{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hay4hi/miniconda3/envs/mad-rag-env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/hay4hi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/hay4hi/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from transformers import DPRQuestionEncoder, DPRContextEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoderTokenizer\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import torch\n",
    "\n",
    "from  chunking import Chunking\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"cuda:1\"\n",
    "# Download NLTK data\n",
    "# I had to export as an env var where the data were downloaded : export NLTK_DATA=/home/hay4hi/nltk_data\n",
    "nltk.set_proxy('http://rb-proxy-de.bosch.com:8080')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Load retriever models and tokenizers\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base').to(device)\n",
    "context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base').to(device)\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "\n",
    "\n",
    "# Bart does not work so great for question answering\n",
    "# Load generator model and tokenizer\n",
    "#llm_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large').to(device)\n",
    "#llm_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "\n",
    "# GPT2 does not work so great for question answering\n",
    "#from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "#llm_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "#llm_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load pre-trained T5 model and tokenizer\n",
    "llm_model = T5ForConditionalGeneration.from_pretrained('t5-11B')  # You can also use 't5-base' or 't5-large'\n",
    "llm_tokenizer = T5Tokenizer.from_pretrained('t5-11B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webpage downloaded and processed successfully.\n",
      "Cleaned webpage content saved to 'friends_S8_E4.txt'.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# URL of the webpage\n",
    "url = 'https://fangj.github.io/friends/season/0804.html'\n",
    "\n",
    "# Headers to mimic a browser\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    webpage_content = response.text\n",
    "    soup = BeautifulSoup(webpage_content, 'html.parser')\n",
    "    \n",
    "    # Extract the text content\n",
    "    text_content = soup.get_text(separator=' ', strip=True)  # Use space as a separator instead of newlines\n",
    "    cleaned_text = ' '.join(text_content.split())  # Remove excessive spaces and newlines\n",
    "    print(\"Webpage downloaded and processed successfully.\")\n",
    "    \n",
    "    # Save the cleaned text content to a file\n",
    "    with open('friends_S8_E4.txt', 'w') as file:\n",
    "        file.write(text_content)\n",
    "    print(\"Cleaned webpage content saved to 'friends_S8_E4.txt'.\")\n",
    "else:\n",
    "    print(f\"Failed to download webpage. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text document\n",
    "with open('friends_S8_E4.txt', 'r') as file:\n",
    "    text_data = file.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = Chunking()\n",
    "\n",
    "# Step 1: Divide the text into chunks (e.g., sentences)\n",
    "#chunker.chunking_into_sentences(large_document)\n",
    "chunker.chunking_sliding_window(text_data, window_size=10, stride=1)\n",
    "chunks = chunker.chunks\n",
    "\n",
    "# Step 2: Encode the chunks using the context encoder\n",
    "chunk_embeddings = [context_encoder(**context_tokenizer(chunk, return_tensors='pt').to(device)).pooler_output for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input query\n",
    "query = \"what is the backpacking story ?\"\n",
    "\n",
    "# Step 3: Encode the query using the question encoder\n",
    "query_embedding = question_encoder(**question_tokenizer(query, return_tensors='pt').to(device)).pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the retrieved chunk is :  It’s this story I\n",
      "came up with, very romantic. I swear any woman that hears it; they’re like putty. Ross: Really? Well then tell it to me. Joey: Okay. Now you’re gonna want to have sex with me when you hear it, but\n",
      "you have to remember it is just the story. Ross: (sarcastic) I’ll try to control myself. Joey: Okay. (Clears throat) Years ago, when I was backpacking across Western\n",
      "Europe… Ross: (laughs) You were backpacking across Western Europe? Joey: Have a nice six more months Ross!\n"
     ]
    }
   ],
   "source": [
    "similarities = [torch.cosine_similarity(query_embedding, chunk_embedding) for chunk_embedding in chunk_embeddings]\n",
    "retrieved_chunk = chunks[torch.argmax(torch.tensor(similarities))]\n",
    "print(\"the retrieved chunk is : \", retrieved_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q : what is the backpacking story ?, knowing that It’s this story I\n",
      "came up with, very romantic. I swear any woman that hears it; they’re like putty. Ross: Really? Well then tell it to me. Joey: Okay. Now you’re gonna want to have sex with me when you hear it, but\n",
      "you have to remember it is just the story. Ross: (sarcastic) I’ll try to control myself. Joey: Okay. (Clears throat) Years ago, when I was backpacking across Western\n",
      "Europe… Ross: (laughs) You were backpacking across Western Europe? Joey: Have a nice six more months Ross!, A: \"It\" is a song about a young girl who is\n",
      "Ross: I don't know why I said it but I know you like it! A young guy: What do you mean? Ross : (giggle) Ross, if\n"
     ]
    }
   ],
   "source": [
    "## FOR GTP2\n",
    "\n",
    "#query = \"who is joey in friends\"\n",
    "the_prompt = f\"Q : {query}, knowing that {retrieved_chunk}, A:\"\n",
    "prompt_tokens = llm_tokenizer.encode(the_prompt, return_tensors='pt').to(device)\n",
    "answer_tokens = llm_model.generate(\n",
    "        prompt_tokens,\n",
    "        max_length=200,  # Adjust as needed\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        temperature=1,\n",
    "        top_k=10,\n",
    "        top_p=0.95,\n",
    "        do_sample=True\n",
    "    )\n",
    "response = llm_tokenizer.decode(answer_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years ago, when I was backpacking across Western Europe\n"
     ]
    }
   ],
   "source": [
    "#query = \"can you summarize the backpacking story ?\"\n",
    "input_text = f\"question: {query}  context: \"\n",
    "input_text += retrieved_chunk\n",
    "input_tokens = llm_tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Generate the answer\n",
    "output_tokens = llm_model.generate(input_tokens, \n",
    "                               max_length=200,  # Adjust as needed\n",
    "                                num_return_sequences=1,\n",
    "                                no_repeat_ngram_size=2,\n",
    "                                temperature=2.0,\n",
    "                                top_k=10,\n",
    "                                top_p=0.95,\n",
    "                                do_sample=True)\n",
    "answer = llm_tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mad-rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
